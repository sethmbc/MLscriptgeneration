{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'House.txt'\n",
    "\n",
    "with open(data_dir) as f:\n",
    "    data = f.read()\n",
    "    \n",
    "data = data[81:].lower()\n",
    "\n",
    "# seperate the punchuations from the words\n",
    "punch = ['.', '[', ']', '(', ')', ';', ':', \"'\", '/', '\"', ',', '?', '*', '!', '-', '$', '%', '&', '\\n']\n",
    "\n",
    "for i in punch:    \n",
    "    data = data.replace(i, ' ' + i + ' ')\n",
    "    \n",
    "data = data.replace('\\n', '<NEWLINE>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(text):\n",
    "    \n",
    "    vocab_to_int = dict()\n",
    "    int_to_vocab = dict()\n",
    "    \n",
    "    vocab = Counter()\n",
    "    for word in text.split():\n",
    "        vocab[word] += 1\n",
    "        \n",
    "    index = 0    \n",
    "    for word in vocab:\n",
    "        vocab_to_int[word] = index\n",
    "        int_to_vocab[index] = word\n",
    "        index += 1\n",
    "        \n",
    "    return vocab, vocab_to_int, int_to_vocab\n",
    "\n",
    "vocab, vocab_to_int, int_to_vocab = get_vocab(data)\n",
    "\n",
    "# converting text into int\n",
    "text_int = []\n",
    "\n",
    "for word in data.split():\n",
    "    text_int.append(vocab_to_int[word])\n",
    "    \n",
    "text_int = np.array(text_int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alking on the telephone without a shirt on .  ]  <NEWLINE>  <NEWLINE> brandon :  i didn’t sleep well last night ,  and i woke up with a scratchy throat .  i just don’t feel so good .   [ pause ]  uh ,  cough ,   [ clears his throat ]  yeah ,  i’m ,  i’m ,  a bit of an upset stomach too ,  and i thin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alking on the telephone without a shirt on .  ]  <NEWLINE>  <NEWLINE> brandon :  i didn’t sleep well last night ,  and i woke up with a scratchy throat .  i just don’t feel so good .   [ pause ]  uh ,  cough ,   [ clears his throat ]  yeah ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:240].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  1  7  8  9  9 10 11 12 13 14 15 16 17 18 19 12 20\n",
      " 21 22  5 23 24  7 12 25 26 27 28 29  7 30 31  8 32 18 33 18 30 34 35 24\n",
      "  8 36 18 37 18 37 18  5 38 39 40 41 42 43 18 19 12 44 37 45  5 46 18 37\n",
      " 25 47 12 48 49 50  7 30 51  8 52 18 53 18 36 37 54 55 49 56 57 58 59  7\n",
      " 30 31  8 60]\n"
     ]
    }
   ],
   "source": [
    "print(text_int[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9691, 200)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "\n",
    "def get_training_data(data, seq_len):\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(0, len(data)-seq_len):\n",
    "        \n",
    "        x = data[i:i+seq_len]\n",
    "        y = data[i+1:i+seq_len+1]\n",
    "        \n",
    "        x_train.append(np.array(x))\n",
    "        y_train.append(np.array(y))\n",
    "        \n",
    "    return x_train, y_train\n",
    "  \n",
    "x, y = get_training_data(text_int, seq_len)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 300)         437700    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                [(None, None, 128), (None 219648    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                [(None, None, 128), (None 131584    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                [(None, None, 128), (None 131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 1459)        188211    \n",
      "=================================================================\n",
      "Total params: 1,108,727\n",
      "Trainable params: 1,108,727\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding = 300\n",
    "lstm_size = 128\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "inp = Input((None,))\n",
    "\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=embedding)\n",
    "lstm1 = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "lstm2 = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "lstm3 = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "dense = Dense(vocab_size)\n",
    "\n",
    "net = embed(inp)\n",
    "net, h1, c1 = lstm1(net)\n",
    "net, h2, c2 = lstm2(net)\n",
    "net, h3, c3 = lstm3(net)\n",
    "out = dense(net)\n",
    "\n",
    "model = Model(inp, out)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "9691/9691 [==============================] - 258s 27ms/step - loss: 7.0984 - accuracy: 0.0686\n",
      "Epoch 2/4\n",
      "9691/9691 [==============================] - 206s 21ms/step - loss: 5.8266 - accuracy: 0.0979\n",
      "Epoch 3/4\n",
      "9691/9691 [==============================] - 200s 21ms/step - loss: 5.0717 - accuracy: 0.1175\n",
      "Epoch 4/4\n",
      "9691/9691 [==============================] - 201s 21ms/step - loss: 4.5717 - accuracy: 0.1722\n",
      "<keras.callbacks.callbacks.History object at 0x1a4156af28>\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.optimizer = 0.01\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.fit(x, y, batch_size=128, epochs=4, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states = [Input((lstm_size,)) for i in range(6)]\n",
    "\n",
    "inference = embed(inp)\n",
    "inference, h1, c1 = lstm1(inference, initial_state=init_states[:2])\n",
    "inference, h2, c2 = lstm2(inference, initial_state=init_states[2:4])\n",
    "inference, h3, c3 = lstm3(inference, initial_state=init_states[4:6])\n",
    "inf_out = dense(inference)\n",
    "\n",
    "states = [h1, c1, h2, c2, h3, c3]\n",
    "inf_model = Model([inp]+init_states, [inf_out]+states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(length, start):\n",
    "    \n",
    "    states = [np.zeros((1, lstm_size)) for i in range(6)]\n",
    "\n",
    "    token = np.zeros((1,1))\n",
    "    token[0,0] = start\n",
    "    text = int_to_vocab[start] + ' '\n",
    "    \n",
    "    for i in range(length):\n",
    "        \n",
    "        out = inf_model.predict([token]+states)\n",
    "        word = np.argmax(out[0][0,0,:])\n",
    "        text += int_to_vocab[word] + ' '\n",
    "        states = out[1:7]\n",
    "        token[0][0] = word\n",
    "        \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_text(text):\n",
    "    \n",
    "    punch1 = ['.', ':', '!', ';', ')', ']', '?', ',', '%']\n",
    "    for i in punch1:\n",
    "        text = text.replace(' '+i, i)\n",
    "        \n",
    "    punch2 = ['[', '(', '$']    \n",
    "    for i in punch2:\n",
    "        text = text.replace(i+' ', i)\n",
    "        \n",
    "    punch3 = [\"'\", '-']    \n",
    "    for i in punch3:\n",
    "        text = text.replace(' '+i+' ', i)\n",
    "        \n",
    "    text = text.split('<NEWLINE>')  \n",
    "    for line in text:\n",
    "        if len(line):\n",
    "        \n",
    "            return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alking [the the the,,,,,,, [[has to to,,,,,, [[off off to]]. ', ' ', '::: you we’re we’re on on on on on on on]] ', ' ', ' did:: are!! start start on on on on on,,,, so – – –:: [the the the the the the do do do do do do see see []]],, – – –::,,, we’re [[the the the the the]] do do do do do to see you?? [', ' ', '::,,,,. [[[to the]]]] hey hey hey hey hey we’re ', ' house walks walks.. ', ' ', ' ', '::: has has,,,,,, [[[[the]]]] to hey hey hey hey hey ', ' ', ':: house walks walks.. ', ' ', ' ', '::: has,, on ']\n"
     ]
    }
   ],
   "source": [
    "generated_text = extract_text(200, 0)\n",
    "generated_text = post_process_text(generated_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
